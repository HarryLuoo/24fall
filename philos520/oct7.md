# Comprehensive Summary of "Modelling the Deep Past" by Adrian Currie

## **Introduction**

In "Modelling the Deep Past," Adrian Currie delves into the intricate methodologies employed by paleoscientists to reconstruct and understand Earth's extensive history. Currie critically examines the prevailing philosophical frameworks that prioritize trace-based reasoning—inferring past events from observable remnants like fossils—and argues that these frameworks inadequately account for the pervasive and essential role of modeling in paleoscientific endeavors. He posits that models are not merely supplementary tools but are fundamental to piecing together both the actualities and possibilities of the deep past, thereby bridging gaps imposed by the vast temporal scales and the inevitable decay of information over millions of years.

## **Abstract**

Currie's central thesis challenges traditional philosophical analyses that predominantly focus on trace-based reasoning within paleosciences. He asserts that such accounts fail to encapsulate the critical and ubiquitous role of modeling practices. Models, according to Currie, are indispensable not only for inferring historical facts but also for exploring potential scenarios, thus facilitating a deeper and more comprehensive understanding of the past. He introduces the concept of "phenomena-driven" modeling, where models are intricately linked with empirical data, ensuring that the possibilities they explore are directly relevant to the specific historical investigations at hand.

## **1. Introduction**

Currie begins by highlighting the epistemic challenges faced by paleoscientists, primarily the scarcity of information due to the immense timescales involved and the consequent decay of evidence. To mitigate these challenges, paleoscientists adopt "methodologically omnivorous" strategies, employing a diverse array of methods to extract knowledge from limited and often degraded data. Among these strategies, the use of models stands out as particularly crucial. Traditional philosophical accounts, which emphasize trace-based reasoning—drawing inferences from fossilized remains and other traces—are critiqued for overlooking the integral role of modeling. Currie contends that historical sciences like paleontology are less about temporal sequences and more about modality, meaning they frequently engage with what could have been, not just what was.

## **2. Models & Traces**

### **2.1. Trace-Based Reasoning**

Trace-based reasoning involves making inferences about past events, entities, and processes based on observable traces such as fossils, tracks, and burrows. This method relies heavily on background theories that explain how these traces form. For instance, understanding the processes of fossilization allows scientists to infer the existence and characteristics of extinct organisms. Philosophers like Carol Cleland have framed historical science as primarily employing "smoking gun" reasoning, where surprising correlations between present observations lead to hypotheses about the past, which are then tested by seeking further traces. Derek Turner, advocating an anti-realist perspective, argues that trace-based reasoning in historical sciences is fundamentally underdetermined due to the decay of traces and the inability to intervene in the past, leading to significant gaps in knowledge.

### **2.2. Modelling as Strategy**

Currie shifts the focus to modeling as a strategic scientific practice. Philosophers such as Michael Weisberg and Peter Godfrey-Smith define models based on their development and usage rather than their intrinsic features, viewing them as indirect strategies to understand target systems. Weisberg introduces the distinction between "abstract direct representation" (ADR), where models directly represent phenomena using empirical data, and the modeller’s strategy, which involves creating simplified, manipulable proxies inspired by the target system to explore and understand its dynamics.

A pivotal case study illustrating this is the MBL (Macroevolutionary Branching Lineage) model developed in the 1970s. This stochastic branching model simulates evolutionary patterns by allowing lineages to speciate, remain unchanged, or go extinct at each time step with equal probability. By manipulating probabilities and introducing deterministic elements, paleontologists like Raup and Sepkoski used the MBL model to identify macroevolutionary events that could not be explained by randomness alone, thereby pinpointing the necessity of more complex processes like natural selection.

### **2.3. Philosophy of Paleoscientific Modeling**

Currie surveys various philosophical contributions that emphasize the role of models in historical sciences. Functional morphology, for example, employs simulations to infer locomotive capabilities of extinct animals, such as determining whether Spinosaurus was adapted for aquatic pursuit. Archaeology uses models to interpret cultural spread and subsistence activities, often integrating legacy data with new evidence through model-based approaches. Paleoclimatology leverages complex computational models to represent climatic factors, aiding in the reconstruction of Earth's climatic history.

Philosophers have highlighted how models in these disciplines serve multiple functions: they correct and refine fossil data, navigate scale differences, and integrate old data with contemporary evidence. Currie underscores that models act as epistemic artifacts, facilitating the understanding of both actual historical events and their possible variations, thereby enhancing the robustness of historical reconstructions.

## **3. Two Models of the Deep Past**

Currie presents two detailed case studies to illustrate the application and significance of modeling in paleoscientific research.

### **3.1. Dinosaur Tails**

The first case study examines the work of Ibrahim et al. (2020), who investigated the locomotion of Spinosaurus, the largest known theropod dinosaur. By creating physical models of various theropod tails, including that of Spinosaurus, and testing their propulsion efficiency in water, they aimed to determine whether Spinosaurus was adapted for aquatic pursuit. The experiments demonstrated that Spinosaurus' tail could generate significantly more thrust and was more efficient than those of other theropods, supporting the hypothesis that it was adapted for swimming and hunting aquatic prey.

Philosophically, this study exemplifies the "Kon-Tiki Experiment," wherein models are used to test specific capacity hypotheses about extinct organisms. The experiments by Ibrahim et al. establish not only the possibility that Spinosaurus could swim effectively but also provide modal insights into the relationship between tail morphology and aquatic propulsion. This approach underscores how models can probe the capacities and potential behaviors of extinct species, thereby enriching our understanding of their ecology and lifestyle.

### **3.2. Avalonian Ecological Communities**

The second case study focuses on Mitchell et al. (2019), who utilized Spatial Point Process Analysis (SPPA) to study the spatial distribution of Ediacaran taxa in Avalonian assemblages. SPPA compares the actual spatial distribution of taxa with a stochastic null model to determine whether the distribution is neutral (stochastic) or structured by ecological niches (deterministic processes). The findings indicated that Avalonian communities were predominantly structured by neutral processes, a stark contrast to modern marine ecosystems typically governed by niche differentiation.

This study illustrates how modeling can be employed to test ecological regularities and understand community dynamics in the deep past. By situating Avalonian communities within the framework of SPPA, Mitchell et al. were able to explore whether the observed spatial patterns could arise purely by chance or required underlying ecological processes. Their conclusion—that neutral processes dominated—challenges existing ecological theories and prompts further investigation into the unique conditions of Ediacaran ecosystems. This exemplifies the phenomena-driven approach, where models are directly informed and constrained by empirical observations, ensuring their relevance and applicability to the specific historical phenomena under study.

## **4. Modality & Modeling in Historical Science**

### **4.1. Possible Pasts**

Currie emphasizes that historical sciences are inherently concerned with both the actual and the possible past. Understanding what could have happened is as crucial as knowing what did happen, as it allows scientists to reconstruct events within a broader context of possibilities. Models play a critical role in this process by exploring capacity hypotheses and testing regularities that underpin historical narratives.

For instance, the study on Spinosaurus' tail by Ibrahim et al. not only supports the actual scenario of aquatic pursuit but also delineates the range of possible locomotion capabilities based on tail morphology. Similarly, the SPPA approach used by Mitchell et al. explores the range of possible ecological configurations, thereby enriching the understanding of Avalonian communities beyond mere trace analysis.

### **4.2. Phenomena-Driven Modeling**

Currie introduces the concept of "phenomena-driven" modeling, which contrasts with traditional, theory-driven modeling approaches. In phenomena-driven modeling, the construction and refinement of models are directly informed by empirical data, ensuring that the exploration of possibilities remains grounded in observed phenomena. This iterative process involves continuous validation and adjustment of models based on empirical findings, enhancing their relevance and accuracy.

In the case studies presented, both Ibrahim et al. and Mitchell et al. exemplify phenomena-driven modeling. Ibrahim et al. tailored their tail models to match fossil reconstructions and empirical constraints, while Mitchell et al. closely aligned their SPPA models with detailed spatial and taxonomic data from Avalonian assemblages. This approach ensures that the models are not abstract explorations of theoretical possibilities but are instead deeply intertwined with the specific empirical contexts they seek to understand.

Currie argues that this phenomena-driven approach necessitates a different modeling strategy than that described by Weisberg and Godfrey-Smith. Instead of building abstract proxies to understand general theories, phenomena-driven models are iteratively refined to align closely with specific empirical data, ensuring that the possibilities they explore are directly relevant to the historical phenomena at hand.

## **5. Conclusion**

Adrian Currie's exploration in "Modelling the Deep Past" underscores the indispensable role of modeling in historical sciences, particularly paleontology. He argues that while trace-based reasoning remains foundational, modeling practices are equally critical for bridging epistemic gaps and exploring the rich landscape of historical possibilities. Models enable paleoscientists to reconstruct not just what existed but also how it might have functioned and interacted within its environment.

Currie advocates for a philosophical reevaluation that moves beyond trace-centric accounts to incorporate the nuanced and pervasive role of modeling in historical reasoning. He highlights the necessity of phenomena-driven modeling, where models are intricately connected with empirical data, ensuring their relevance and enhancing their explanatory power. This approach not only enriches our understanding of the deep past but also reveals the modal dimensions of historical sciences, positioning them as disciplines deeply concerned with both actuality and possibility.

## **Key Takeaways**

1. **Integration of Models and Traces**: Traditional trace-based reasoning, while foundational, is insufficient on its own. Models complement trace analysis by exploring and testing possible historical scenarios, thereby providing a more holistic understanding of the past.

2. **Phenomena-Driven Modeling**: This approach ensures that models remain closely tied to empirical data, enhancing their relevance and reliability. It contrasts with more abstract, theory-driven modeling strategies by emphasizing iterative refinement based on specific historical phenomena.

3. **Modal Dimensions of Historical Science**: Historical sciences like paleontology engage deeply with modality—what could have happened—not just temporality—what did happen. Models are essential tools for navigating this modal landscape, enabling scientists to reconstruct and understand a wide array of possible historical narratives.

4. **Philosophical Implications**: Currie's work calls for philosophical frameworks to expand beyond trace-based reasoning, recognizing the crucial and multifaceted role of modeling in historical sciences. This expansion can lead to more accurate and comprehensive accounts of scientific methodologies and epistemologies in disciplines concerned with the deep past.

## **Implications for Future Research**

Currie's analysis invites a broader philosophical engagement with the methodologies of historical sciences. Future research could focus on developing more sophisticated philosophical models that fully integrate the role of modeling in historical reasoning. Additionally, interdisciplinary collaborations between philosophers and paleoscientists could yield deeper insights into how models are constructed, validated, and utilized to interpret the deep past. Methodological innovations in modeling techniques, particularly those that better integrate empirical data with theoretical explorations of possibility, could further enhance the robustness and accuracy of historical reconstructions.

---
### Summary of "Using Models to Correct Data: Paleodiversity and the Fossil Record" by Alisa Bokulich

#### Introduction

Alisa Bokulich’s paper, published in *Synthese* (2021), addresses a relatively underexplored topic in the philosophy of science: the construction and correction of data models, particularly in paleontology. While much philosophical work has focused on theoretical models, Bokulich turns her attention to *data models*—processed versions of raw data that have been subjected to various statistical and analytical techniques. Specifically, she examines how paleontologists use models to correct paleodiversity data derived from the fossil record, which is inherently incomplete and biased. Bokulich argues that the epistemic reliability of data models is not determined by their "purity" (i.e., how unprocessed they are) but by their *fidelity*—how well they capture the signal of interest. She further contends that fidelity is a matter of degree and can be improved through post hoc model-based corrections. Finally, she emphasizes that data models, like theoretical models, should be evaluated as *adequate-for-purpose* rather than as universally accurate or inaccurate.

#### Historical Context: The Fossil Record and Its Biases

The fossil record is a crucial source of data for understanding the history of life on Earth, but it is also highly incomplete and biased. Early geologists like Charles Lyell and Charles Darwin recognized that the fossil record is not a perfect or systematic record of past life. Darwin, in particular, was concerned that the gaps in the fossil record could be used as evidence against his theory of evolution by natural selection. He devoted an entire chapter in *On the Origin of Species* to the "imperfection of the geological record," identifying several factors that bias the fossil record, such as taphonomic filters (which determine what types of organisms are likely to be preserved), geological processes (which destroy or obscure fossils), and anthropogenic biases (such as the uneven geographical distribution of fossil collection efforts).

In the 1970s, the field of paleobiology underwent a revolution, as paleontologists began to develop quantitative methods and computer simulations to analyze large-scale patterns in the fossil record. One of the key figures in this revolution was Jack Sepkoski, who compiled a massive database of fossil data and produced the famous "Sepkoski curve," which tracks marine biodiversity over time. Sepkoski’s work highlighted the need to correct for biases in the fossil record, leading to the development of various model-based data correction techniques.

#### Three Approaches to Correcting Paleodiversity Data

Bokulich identifies three main approaches that paleontologists use to correct paleodiversity data: subsampling, residuals, and phylogenetic methods. Each of these approaches involves the use of models to mitigate the biases in the fossil record and improve the fidelity of the data.

1. **Subsampling Approaches**: 
   Subsampling methods aim to correct for biases that arise from differences in sample size. The classical rarefaction method, introduced by David Raup in 1975, assumes that a "fair" sample is one in which all samples have roughly the same number of individuals. However, John Alroy has argued that this method is inadequate because it does not account for the fact that when diversity is high, more sampling is needed to get an accurate picture of the species present. Alroy proposes an alternative method called *shareholder quorum subsampling* (SQS), which tracks the "coverage" of the data set (i.e., the proportion of the total population represented by the sample). SQS is a significant improvement over classical rarefaction, but it still has limitations, such as the assumption of random sampling.

2. **Residuals Approaches**: 
   The residuals method attempts to separate the biological signal from the geological signal in the fossil record. The geological signal is understood as the amount of sedimentary rock available for fossilization, which varies across different time periods. By modeling the effect of the geological signal and subtracting it from the raw diversity data, paleontologists can obtain a corrected estimate of biological diversity. This method has been tested using computer simulations, which show that the corrected data provides a better representation of paleodiversity than the raw data.

3. **Phylogenetic Approaches**: 
   Phylogenetic methods use cladistic analysis to infer ancestral relationships among taxa and fill in gaps in the fossil record. For example, if two taxa are determined to be sister taxa, and one appears earlier in the fossil record than the other, a "ghost lineage" is inferred to extend the range of the later-appearing taxon back to the time of its sister taxon. Phylogenetic methods can also account for "Lazarus taxa," which disappear from the fossil record for long periods but reappear later. As with the other methods, the reliability of phylogenetic corrections can be tested using computer simulations.

#### Philosophical Implications: Fidelity and Adequacy-for-Purpose

Bokulich draws several important philosophical conclusions from this case study of paleodiversity data correction:

1. **Purity vs. Fidelity**: 
   Contrary to the intuition that "purer" data (i.e., less processed data) is more reliable, Bokulich argues that it is the *fidelity* of the data that matters. Fidelity refers to how well the data captures the signal of interest, and it can be improved through model-based corrections. The goal is not to obtain "pure" data but to remove artefactual elements and reduce noise.

2. **Fidelity as a Matter of Degree**: 
   Fidelity is not an all-or-nothing property; it is a matter of degree. A data model can do a better or worse job of capturing the biological signal, and the goal is to improve fidelity incrementally.

3. **Vicarious Control**: 
   Fidelity can be improved not only through physical control (e.g., isolating variables during data collection) but also through *vicarious control*—the use of models to correct data after it has been collected. This is particularly important in fields like paleontology, where it is often impossible to control for all sources of bias during data collection.

4. **Adequacy-for-Purpose**: 
   Data models, like theoretical models, should be evaluated as *adequate-for-purpose*. A data model that is adequate for one purpose (e.g., identifying large-scale patterns in biodiversity) may not be adequate for another purpose (e.g., determining whether dinosaurs were in decline before the Chicxulub impact). The adequacy of a data model depends on the specific hypotheses it is being used to test.

#### Data Models at Different Levels of the Hierarchy

Bokulich emphasizes that model-based corrections are not limited to abstract, global data sets like the Sepkoski curve. They are also used at lower levels of the data-model hierarchy, such as the preparation of fossil specimens. Fossil preparators, who remove the surrounding rock (the "matrix") from fossils, must make decisions about what is signal and what is noise, and these decisions are influenced by the theoretical questions the paleontologist is trying to answer. Thus, even at the level of individual fossil specimens, data models are constructed with a particular purpose in mind.

#### Conclusion

Bokulich’s paper sheds light on the complex process of constructing and correcting data models in paleontology. She argues that the epistemic reliability of data models depends not on their purity but on their fidelity, which can be improved through model-based corrections. Moreover, data models should be evaluated as adequate-for-purpose, rather than as universally accurate or inaccurate. This case study of paleodiversity data correction provides valuable insights into the broader philosophical issues surrounding data models in science, highlighting the iterative process by which scientists improve the fidelity of their data and the importance of assessing data models in relation to specific theoretical goals.

#### Key Takeaways:
- **Purity vs. Fidelity**: The epistemic reliability of data models is determined by their fidelity, not their purity.
- **Fidelity as a Matter of Degree**: Fidelity can be improved incrementally through model-based corrections.
- **Vicarious Control**: Models can be used to correct data after it has been collected, improving fidelity.
- **Adequacy-for-Purpose**: Data models should be evaluated based on their adequacy for specific theoretical purposes.
- **Model-Data Symbiosis**: Models and data are in a mutually dependent relationship, and models are essential for making data scientifically useful.

In sum, Bokulich’s paper provides a detailed and philosophically rich analysis of how models are used to correct data in paleontology, offering broader insights into the nature of data models in science.